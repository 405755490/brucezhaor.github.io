---
layout: post
title: 方差分析：穷人家的孩子学不好？
subtitle: 重新认识方差分析
categories : [statistics]
tags : [aov]
date: 2016-1-7
author: "赵炜"
header-img: "/img/post/aov-bg.png"
description: "总有学校说，我们乡下地方，孩子读书的条件差，肯定考不过城里孩子的...那么？事实真的是这样吗？"
output: 
  html_document: 
    css: ~/GitHub/brucezhaor.github.io/assets/css/style.css
---

## 目录
 
1. [背景知识](#section-1)
	+ [原理与推导](#section-2)
	+ [test](#section-3)
	+ [BSD](#section-3)
	+ [FreeBSD & Apple](#freebsd--apple)
	+ [NeXTStep](#nextstep)
	+ [Darwin](#darwin)
	+ [POSIX](#posix)
2. [Unix-like](#unix-like)
	+ [Single Unix Specification](#single-unix-specification)
	+ [Apple iOS](#apple-ios)
	+ [XNU Kernel](#xnu-kernel)
3. [Linux](#linux)
	+ [Linux Kernel](#linux-kernel)
	+ [GNU Project](#gnu-project)
	+ [Android](#android)
	+ [Android Kernel](#android-kernel)
	+ [Android ROM](#android-rom)
	+ [Chrome OS](#chrome-os)
 
 
与统计和数据分析打交道的人，想必对`方差分析`一点都不陌生，但是真正懂得其原理的人又有多少呢？在写这篇文章之前，作为统计学专业毕业的人，表示对方差分析也只是一知半解，有时候还会出现误用的情况。为了写这篇文章，我又重拾《数理统计》这本书，经过一番推导和查阅相关资料后，总算是有了一定的了解。于是打算写这么一篇文章来与大家分享方差分析的原理以及用法，最后给出实际的例子。

## 背景知识

### 原理与推导

> 没有公式就没有伤害   

为了不给大家造成伤害，我尽量不照搬公式，用人话来解释清楚。

#### 方差分析的起源 

一般两个样本的均值检验，现实中最常用的是**t检验**。因为t检验具有具有以下性质： 

* 两总体方差未知时，可以使用；
* 非正态总体，大样本情况下可以使用；

而**Z检验**（又叫U检验）的前提条件就是两样本的**总体方差已知**，一般情况下总体的方差是不知道的，用样本的方差去估计个人认为偏差很大，所以Z建议现实生活中基本用不上。当然，如果你精通抽样技术（很难），能够计算无偏的总体方差估计，想必早就对这些基本方法不屑一顾了。

刚说了**t检验**只适用于两个样本的均值检验，那如果涉及到多个样本的均值检验呢？有人说:“两两t检验不就行了吗，虽然麻烦了一点，但还不是能做出来的，根本用不着新方法吧。” 

假设有三个样本要对其均值进行检验是否相等，那么就要比较 $$C_3^2 = 3$$ 次了。关键问题是：若t检验的显著性水平 $$\alpha = 0.05$$,那么三次比较的显著性水平为$$1 - (1-\alpha)^3 = 0.1426$$,远远超过当初的设定的0.05，犯第一类错误（弃真）概率大大加大。所以对于多样本的均值比较检验，应采用方差分析


#### 单因素方差分析的基本原理（可以直接跳过）

讲到统计学原理之类的东西，总是少不了公式，要不然会显得不够专业。但是考虑到大多数人看到公式就会直接略过，所以这部分完了之后我会写一个方差分析思想的总结，毕竟思想是最重要的东西。

构建方差分析的基础是F分布，也就是开头见到的那个图。那具体是怎么构建F分布的呢？
在公式推导前，先上一个表格，有助于对公式的理解。假设一个实验有C个水平（横轴），R个观测样本（纵轴）。实验表格数据如下表所示,一共有$$n_r 行和n_c列$$：

水平| A1|A2|A3|...|\\(A_c\\)|
----|---|--|---|---|---|
1|	\\(X_{11}\\)|	\\(X_{12}\\)|	\\(X_{13}\\)|	…|	\\(X_{1c}\\)|
2|	\\(X_{21}\\)|	\\(X_{22}\\)|	\\(X_{23}\\)|	…|	\\(X_{2c}\\)|
3|	\\(X_{31}\\)|	\\(X_{32}\\)|	\\(X_{33}\\)|	…|	\\(X_{3c}\\)|
…|	\\(X_{…}\\)|	\\(X_{…}\\)|	\\(X_{…}\\)|	…|	\\(X_{…}\\)|
r|	\\(X_{r1}\\)|	\\(X_{r2}\\)|	\\(X_{r3}\\)|	…|	\\(X_{rc}\\)|
列平均值| \\( \overline{X_{.1}}\\)| \\( \overline{X_{.2}}\\)| \\( \overline{X_{.3}}\\)| …| \\( \overline{X_{.c}}\\)|

假设条件：**各个水平$$A_1,A_2,...,A_c$$下样本$$X_{1j},X_{2j},...,X_{rj},j=1,2,...,c.$$来自均值$$u_j$$和方差$$\sigma^2$$未知的C个正态分布，且样本观测值之间相互独立。**这包含了三个条件：
1. 各样本的独立性；
2. 随机误差$$X_{ij} - \mu_j = \varepsilon_{ij} \sim N(0,\sigma^2)$$；
3. 同方差性，各水平下样本具有相同的方差。

这个假设是一切分析的基础，否则没法做下去了。令$$ \alpha_j = \mu_j - \mu, 其中 \mu =\frac{1}{c} \sum \mu_j $$。$$\alpha_j 为水平 A_j$$的效应。
转化为公式：

$$ X_{ij} = \mu + \alpha_j + \varepsilon_{ij}; \varepsilon_{ij} \sim N(0,\sigma^2); \alpha_1 + \alpha_2 + ... +\alpha_c =0$$

总偏差平方和：

$$ SST = \sum{ \sum{ (X_{ij} - \overline{X_{..}})^2}} , $$  

其中  

$$  \overline{X_{..}} = \frac{\sum{\sum{X_{ij}}}}{n} , i = 1,2,...,r ; j = 1,2,...,c ; n = r \times c. $$

可以分解为： $$ SST = \sum{ \sum{ (X_{ij} - \overline{X_{..}})^2}} = \sum{ \sum{ (X_{ij} - \overline{X_{.j}})^2}} + \sum{ r (\overline{X_{.j}} - \overline{X_{..}})^2} $$,  
也就是 $$ SST = SSE + SSA $$ 这就是平方和分解定理。(证明过程略掉感兴趣的自行去推导) 其中,SSE 为误差平方和，SSA为水平效应平方和。

从公式可以看出，**误差平方和**是先计算一列的样本观测值与样本平均值的平方和，然后将所有列加起来就是总的误差平方和；**效应平方和**是先计算列样本均值(水平的均值)减去总的均值的平方，乘以行数$$r$$,然后所有列结果加起来就是总的效应平方和。

这个时候我们要用到了假设的部分，这部分需要知道基本统计分布的性质。**拿第一列举例,j=1**. 第一列的误差($$X_{i1} - \overline{X_{.1}}$$)是服从均值为0，标准差为$$\sigma$$的正态分布 $$\sim N(0,\sigma^2)$$ , 那么这一列的误差平方和除以方差 服从.

$$ \frac{\sum_{i=1}^{r}{ (X_{i1} - \overline{X_{.1}})^2}}{\sigma^2} \sim \chi^2(r - 1) $$

自由度为$$r- 1$$的卡方分布。之所以要减去一个自由度是因为 $$\overline{X_{.1}} = \frac{1}{r} \sum_{i=1}^{r}(X_{i1})$$ ，利用了样本信息，故丢失一个自由度。由于**卡方分布的可加性**，那么**所有列的误差平方和**

$$\frac{SSE}{\sigma^2} \sim \chi^2(n - c) ,其中 n=r \times  c . $$ 

因为卡方分布的期望是其自由度，可以知道SSE的期望是

 $$\frac{E(SSE)}{n-c} = \sigma^2$$

方差分析的原假设H0: $$\mu_1=\mu_2 =...=\mu_c$$ ; 备择假设H1: $$\mu_1,\mu_2 ,...,\mu_c $$不全相等。当H0为真时，$$X_{ij}  \sim N(\mu,\sigma^2).$$

$$ \therefore \frac{SST}{\sigma^2} = \frac{\sum{ \sum{ (X_{ij} - \overline{X_{..}})^2}}}{\sigma^2} \sim \chi^2(n-1) $$

由分解定理可知:

$$ \frac{SSA}{\sigma^2} = \frac{\sum_{i=1}^{r}{ (X_{.j} - \overline{X_{..}})^2}}{\sigma^2} \sim \chi^2(r - 1) $$

可以求得：

$$\frac{E(SSA)}{c-1} = \sigma^2$$

当H1为真时，计算得：$$ E(SSA) = (C-1) \sigma^2  + \sum_{j=1}^{c}r\alpha_j^2$$ ，其值跟$$\alpha_j$$水平的效应有关系，并且$$\frac{E(SSA)}{c-1} > \sigma^2$$； 而不管H0是否为真,$$E(SSE) =(n-c) \sigma^2$$。

故可以构建检验统计量-基于F分布，SSE/(N-C)作为分母，SSA/(C-1) 作为分子。基于给定的显著水平$$\alpha$$，拒绝域为：

$$ F = \frac{SSA/(C-1)}{SSE/(N-C)} \sim F(C-1,N-C) \ge F_{\alpha}(C-1,N-C) $$

若计算的F值大于或等于$$F_{\alpha}(C-1,N-C)$$，则拒绝原假设，认为各水平的均值不相等。

#### 方差分析的思想

我个人觉得方差分析的精髓在于将总的差异来源分解成了随机误差和效应误差（其实效应误差是含有随机误差的，见参考资料1的P23），当效应误差大于一定的随机误差时，可以认为引起差异的原因大部分在于水平变量的控制。F分布解决了如何判断问题，是由英国统计学家[R.A.Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher)提出，并以其姓氏的第一个字母命名的。还可以将随机误差理解为组内差异，将效应误差理解组间差异。方差分析一般的结果呈现是方差分析表的形式：

方差来源| 平方和 | 自由度 | 均方 | F比值 | p值
---------|--------|---------|------|-------|-----
因素A | SSA | C-1 | MSA=SSA/(C-1) | F= MSA/MSE |
误差 | SSE | N-C| MSE=SSE/(N-C)| 
总合 | SST | N-1| 

**请牢记方差分析的假设条件：1.样本独立；2.误差服从正态分布；3.各试验水平方差近似相等。** 前面两个条件一般都比较好满足，最后一个同方差性比较难满足。很多人在做方差分析前都没有对样本进行检验，拿起数据就算，这样得出的结果是不可信的。有人会问如果条件都不满足，那怎么办? 我的检验是可以尝试非参数的方法，比如kruskal-wallis单因素方差分析，Friedman秩方差分析法。



#### test

#### 重复测量



### 正确使用步骤




## 实例分析




### 户籍与成绩



### 经济水平与成绩


### 与非参数方法比较


## 参考文献：
[1]关于方差分析的详细推导请看这里：[四川大学唐嗣政教授手稿P20-P28](http://memory.scu.edu.cn/UploadFiles/jiaoan_pdf/%E5%94%90%E5%97%A3%E6%94%BF-%E5%87%A0%E4%B8%AA%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%E7%9A%84%E6%8E%A8%E5%AF%BC%20%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90%20%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90.pdf)

